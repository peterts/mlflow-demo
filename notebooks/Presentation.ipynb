{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-tsunami",
   "metadata": {},
   "source": [
    "![MLFlow](https://images.squarespace-cdn.com/content/v1/561c001de4b06530bb65f080/1591324514062-057XFNW1NEPYG0V2W0FK/ke17ZwdGBToddI8pDm48kK6mKuWQ1p4ESdjwtW3BA1EUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcFkXF-TzreDwzLgtTwibDCbWtdkc1DXP09IipfwxkgNMvACCePm4cJPaApzbw6Bn2/MLflow-logo-final-white-TM.jpg)\n",
    "\n",
    "# Modellsporing med MLFlow\n",
    "\n",
    "To komponenter/deler som må være på plass for et fungerende oppsett:\n",
    "1. *En \"tracking server\"* - en applikasjon kjørende \"sentralt\" som kan motta resultater fra python-programmer i prosjektet som logger MLFlow-resultater\n",
    "    - Tracking serveren kan også kjøres lokalt, ved behov, på egen maskin\n",
    "2. *En pakke i prosjektet* - og bruk av denne for å logge resultater til tracking serveren\n",
    "\n",
    "Merk: \n",
    "   - MLFlow har flere komponenter enn dette som kan hjelpe med andre deler av arbeidsflyten, men modellsporing er nok den mest brukte (og subjektivt sett nyttige) delen av MLFlow\n",
    "   - Å legge til modellsporing i MLFlow er dessuten ikke spesielt \"inngripende\", og man kan ofte (i stor grad) beholde samme struktur/arkitektur som tidligere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-house",
   "metadata": {},
   "source": [
    "# Først, litt gjenkjennelig ML-kode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-mapping",
   "metadata": {},
   "source": [
    "## Innhenting av data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")\n",
    "len(newsgroups.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-andorra",
   "metadata": {},
   "source": [
    "## Oppretting av en enkel NLP-pipeline\n",
    "\n",
    "Vi lager her en enkel NLP-pipeline som vektoriserer tekstene med TF-IDF, og deretter mater dette til en SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "analyzer = \"word\"\n",
    "stop_words = \"english\"\n",
    "ngram_range = (1, 2)\n",
    "min_df = 3\n",
    "max_df = .8\n",
    "max_features = 10_000\n",
    "alpha = 0.0001\n",
    "max_iter = 1000\n",
    "loss = \"hinge\"\n",
    "penalty = \"l2\"\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer=analyzer,\n",
    "        stop_words=stop_words,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        max_features=max_features\n",
    "    )),\n",
    "    (\"clf\", SGDClassifier(\n",
    "        alpha=alpha,\n",
    "        penalty=penalty,\n",
    "        loss=loss,\n",
    "        max_iter=max_iter,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-theme",
   "metadata": {},
   "source": [
    "# Et \"MLFlow run\"\n",
    "\n",
    "Nå som vi har en utrent pipeline, og data på plass er vi klare for å *kjøre*. Da er det på tide med første kall/integrasjon mot MLFlow. Vi må fortelle MLFlow at vi ønsker et \"run\".\n",
    "\n",
    "Et \"run\" er som navnet sier en *kjøring* - det vil si kjøring av trening og/eller evaluering av en modell. Resultatet av en kjøring vil være et sett med *filer* (\"artefakter\") og *metrikker* som man ønsker å ta vare på og kunne sammenligne på tvers av kjøringer. Knyttet til en kjøring er også *parametere* som beskriver konfigurasjon som påvirker ytelsen til modellen og *tager* som beskriver annen metadata ved modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-merchandise",
   "metadata": {},
   "source": [
    "## Logging av parametere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-antique",
   "metadata": {},
   "source": [
    "Vi kan starte med å logge det vi allerede vet om kjøringen - hvilke parametre som har blitt brukt for å konfigurere modellen. For å logge dette kan vi bruke `mlflow.log_param` (eller `.log_params` for å logge flere på en gang vha en dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param(\"tfidf__analyzer\", analyzer)\n",
    "mlflow.log_param(\"tfidf__stop_words\", stop_words)\n",
    "mlflow.log_param(\"tfdif__ngram_range\", ngram_range)\n",
    "mlflow.log_param(\"tfidf__min_df\", min_df)\n",
    "mlflow.log_param(\"tfdif__max_df\", max_df)\n",
    "mlflow.log_param(\"tfidf__max_features\", max_features)\n",
    "mlflow.log_param(\"clf__alpha\", alpha)\n",
    "mlflow.log_param(\"clf__max_iter\", max_iter)\n",
    "mlflow.log_param(\"clf__loss\", loss)\n",
    "mlflow.log_param(\"clf__penalty\", penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-astronomy",
   "metadata": {},
   "source": [
    "## Tagging\n",
    "\n",
    "*Parametere* er konfigurasjon som påvirker modellen, men vi ønsker ofte å knytte annen type metadata til en kjøring. Dette kan f.eks. være metadata som *docker*-taggen til bildet som ble brukt for kjøringen. For å sette tagger kan vi bruke `mlflow.set_tag` (eller `.set_tags` for å logge flere på en gang vha en dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tag(\"docker_image_tag\", \"a1b2c3d4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-fishing",
   "metadata": {},
   "source": [
    "## Logging av metrikker\n",
    "\n",
    "Vi starter så med å kjøre kryss-validering, for evaluering av modellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from functools import partial\n",
    "from mlflow_demo.metrics import Scorer\n",
    "import numpy as np\n",
    "\n",
    "scorer = Scorer(\n",
    "    precision_weighted=partial(f1_score, average=\"weighted\", zero_division=0),\n",
    "    recall_weighted=partial(f1_score, average=\"weighted\", zero_division=0),\n",
    "    f1_weighted=partial(f1_score, average=\"weighted\", zero_division=0)\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, random_state=419, shuffle=True)\n",
    "\n",
    "target_names = np.asarray(newsgroups.target_names)\n",
    "X, y = newsgroups.data, target_names[newsgroups.target]\n",
    "\n",
    "metrics_per_split = cross_validate(pipeline, X, y, scoring=scorer, cv=cv, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-nancy",
   "metadata": {},
   "source": [
    "Vi har nå følgende metrikker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-loading",
   "metadata": {},
   "source": [
    "Disse kan vi så logge til mlflow med `mlflow.log_metric` (eller `.log_metrics` for å logge flere på en gang vha en dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "for metric_name, metric_values in metrics_per_split.items():\n",
    "    metric_name = metric_name.replace(\"test_\", \"\")\n",
    "    mlflow.log_metric(metric_name, mean(metric_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-message",
   "metadata": {},
   "source": [
    "## Logging av filer/\"artefakter\"\n",
    "\n",
    "Under kjøring skaper man ofte filer som det kan være interessant å ta vare på til senere. Dette kan for eksempel være figurer, rapporter, eller filer som inneholder selve prediksjonene gjort av modellen. Alle filer kan logges til et \"run\". Noen typer, slik som figurer og tekst kan logges med egne funksjoner, resterende filtyper kan logges med den generiske funksjonen `mlflow.log_artifact`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-resort",
   "metadata": {},
   "source": [
    "### Logging av en figur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow_demo.plotting import create_confusion_matrix_fig\n",
    "import plotly.offline as pyo\n",
    "\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "confusion_matrix_fig = create_confusion_matrix_fig(scorer.y_, scorer.y_pred_)\n",
    "confusion_matrix_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_figure(confusion_matrix_fig, \"confusion_matrix.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-leader",
   "metadata": {},
   "source": [
    "### Logging av tekstfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf_report = classification_report(scorer.y_, scorer.y_pred_)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_text(clf_report, \"classification_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-locator",
   "metadata": {},
   "source": [
    "### Logging av modellen\n",
    "\n",
    "Kanskje det viktigste å ta vare på til senere er selve modellen - i hvert fall dersom man bruker MLFlow i et produksjonssystem. MLFlow har støtte for å logge modell-objekter fra stort sett alle større ML-rammeverk. For eventuelle rammeverk som ikke søttes har man dessuten mulughet til å logge modellen som en generisk \"python-funksjon\". Man kan selvsagt også skrive filene på valgfri måte selv, og logge dem via `mlflow.log_artifact`.\n",
    "\n",
    "I denne notebooken har vi brukt scikit-learn, og benytter dermed funksjoner for scikit-learn for logging av modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X, y)\n",
    "pipeline.predict([\"Computers will rule the world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "signature = ModelSignature(\n",
    "    inputs=Schema([ColSpec(\"string\", \"text\")]),\n",
    "    outputs=Schema([ColSpec(\"string\", \"category_name\")])\n",
    ")\n",
    "mlflow.sklearn.log_model(pipeline, \"model\", signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-turkish",
   "metadata": {},
   "source": [
    "### Avslutte en kjøring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-visitor",
   "metadata": {},
   "source": [
    "# Hyperparameter-tuning av modellen\n",
    "\n",
    "En av de virkelige styrkene til MLFlow er å kunne bruke det for å sammenligne kjøringer med ulike parametere. La oss lage en funksjon for å gjennomføre en kjøring gitt et sett av parametere, som logger paramteterne og resultater fra kjøringen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def evaluate_and_log(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        _pipeline = clone(pipeline).set_params(**params)\n",
    "        _scorer = scorer.copy()\n",
    "        \n",
    "        metrics_per_split = cross_validate(_pipeline, X, y, scoring=_scorer, cv=cv)\n",
    "        \n",
    "        for metric_name, metric_values in metrics_per_split.items():\n",
    "            metric_name = metric_name.replace(\"test_\", \"\")\n",
    "            mlflow.log_metric(metric_name, mean(metric_values))\n",
    "            \n",
    "        confusion_matrix_fig = create_confusion_matrix_fig(_scorer.y_, _scorer.y_pred_)\n",
    "        mlflow.log_figure(confusion_matrix_fig, \"confusion_matrix.html\")\n",
    "        \n",
    "        clf_report = classification_report(_scorer.y_, _scorer.y_pred_)\n",
    "        mlflow.log_text(clf_report, \"classification_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-gibraltar",
   "metadata": {},
   "source": [
    "Så definerer vi et rom vi ønsker å søke over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import random\n",
    "\n",
    "params_grid = {\n",
    "    \"tfidf__analyzer\": [\"word\", \"char\", \"char_wb\"],\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2), (1, 3), (3, 3), (4, 4), (5, 5)],\n",
    "    \"tfidf__min_df\": [1, 2, 5, 10],\n",
    "    \"tfidf__max_df\": [0.5, 0.8, 1.0],\n",
    "    \"tfidf__stop_words\": [\"english\", None],\n",
    "    \"tfidf__max_features\": [5000, 10_000, 20_000, None],\n",
    "    \"clf__alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"clf__max_iter\": [100, 500, 1000, 2000, 5000],\n",
    "    \"clf__loss\": [\"log\", \"hinge\"],\n",
    "    \"clf__penalty\": [\"l1\", \"l2\", \"elasticnet\"]\n",
    "}\n",
    "\n",
    "\n",
    "def is_valid_param_combo(params):\n",
    "    analyzer = params[\"tfidf__analyzer\"]\n",
    "    ngram_range_min, _ = params[\"tfidf__ngram_range\"]\n",
    "    if ngram_range_min < 3:\n",
    "        return analyzer == \"word\"\n",
    "    return analyzer.startswith(\"char\")\n",
    "\n",
    "params_list = list(filter(is_valid_param_combo, ParameterGrid(params_grid)))\n",
    "params_samples = random.sample(params_list, 200)\n",
    "params_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-young",
   "metadata": {},
   "source": [
    "## Kjøring av tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-screening",
   "metadata": {},
   "source": [
    "Vi definerer først et nytt annet MLFLow-eksperiment hvor vi ønsker at disse kjøringene skal ende opp. Man kan tenke på et MLFLow-eksperiment som en mappe/samling av kjøringer som man ønsker å kunne sammenligne. Typisk bør dermed alle kjøringene innenfor samme eksperiment være *sammenlignbare* - altså logge samme parametere og metrikker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Simple NLP model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for params in tqdm(params_samples):\n",
    "    evaluate_and_log(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlflow-demo)",
   "language": "python",
   "name": "mlflow-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
